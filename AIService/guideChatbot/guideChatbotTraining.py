import pandas as pd
import torch
import os
from transformers import AutoModelForCausalLM, AutoTokenizer
from datasets import Dataset
from torch.utils.data import DataLoader
from huggingface_hub import login
from tqdm import tqdm

# âœ… Hugging Face ë¡œê·¸ì¸
huggingface_token = os.getenv("HUGGINGFACE_TOKEN")
login(token=huggingface_token)

# âœ… ëª¨ë¸ ë° í† í¬ë‚˜ì´ì € ë¡œë“œ
model_name = "google/gemma-2b"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

# âœ… Mac OSì—ì„œ ì ì ˆí•œ ë””ë°”ì´ìŠ¤ ì„ íƒ
device = torch.device("mps") if torch.backends.mps.is_available() else (
    torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
)
print(f"ğŸ”¹ Using device: {device}")
model.to(device)

# âœ… CSV ë°ì´í„° ë¡œë“œ
try:
    df = pd.read_csv("guideChatbot.csv")
    print("ğŸ”¹ CSV ë°ì´í„° ë¡œë“œ ì™„ë£Œ!")
    print(df.head())

    # âœ… ì§ˆë¬¸-ë‹µë³€ í¬ë§·íŒ…
    df["formatted_text"] = df.apply(lambda row: f"[Q] {row['inputs']} [A] {row['response']}", axis=1)

    # âœ… Dataset ë³€í™˜
    dataset = Dataset.from_pandas(df[["formatted_text"]])

    # âœ… í† í¬ë‚˜ì´ì§• í•¨ìˆ˜ (ê°œì„ ë¨)
    def tokenize_function(examples):
        tokenized = tokenizer(
            examples["formatted_text"],
            padding="max_length",
            truncation=True,
            max_length=128  # âœ… max_length ì¤„ì„
        )
        tokenized["labels"] = tokenized["input_ids"].copy()
        return tokenized

    # âœ… ë°ì´í„° í† í¬ë‚˜ì´ì§• ì ìš©
    tokenized_datasets = dataset.map(tokenize_function, batched=True, remove_columns=["formatted_text"])

    # âœ… ë°ì´í„°ì…‹ì„ í•™ìŠµìš©(train)ê³¼ ê²€ì¦ìš©(eval)ìœ¼ë¡œ ë¶„ë¦¬
    split_data = tokenized_datasets.train_test_split(test_size=0.2)
    train_dataset = split_data["train"]
    eval_dataset = split_data["test"]

    print(f"ğŸ”¹ í•™ìŠµ ë°ì´í„°ì…‹ í¬ê¸°: {len(train_dataset)}")
    print(f"ğŸ”¹ ê²€ì¦ ë°ì´í„°ì…‹ í¬ê¸°: {len(eval_dataset)}")

    # âœ… PyTorch DataLoader ìƒì„±
    def collate_fn(batch):
        collated_batch = {key: torch.tensor([b[key] for b in batch], dtype=torch.long) for key in batch[0]}
        return collated_batch

    train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True, collate_fn=collate_fn)  # âœ… batch_size ì¦ê°€
    eval_dataloader = DataLoader(eval_dataset, batch_size=8, collate_fn=collate_fn)

    # âœ… ì˜µí‹°ë§ˆì´ì € ì„¤ì • (í•™ìŠµë¥  ê°ì†Œ)
    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)  # âœ… lr=2e-5 ë¡œ ì¡°ì •

    # âœ… í•™ìŠµ ë£¨í”„
    num_epochs = 1
    model.train()
    for epoch in range(num_epochs):
        print(f"\nğŸ”¹ Epoch {epoch+1}/{num_epochs} ì‹œì‘...")
        total_loss = 0
        progress_bar = tqdm(train_dataloader, desc=f"Epoch {epoch+1}", leave=False)

        for batch in progress_bar:
            inputs = {k: v.to(device) for k, v in batch.items()}
            outputs = model(**inputs)
            loss = outputs.loss
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            total_loss += loss.item()
            progress_bar.set_postfix({"Batch Loss": loss.item()})

        avg_loss = total_loss / len(train_dataloader)
        print(f"âœ… Epoch {epoch+1}/{num_epochs} ì™„ë£Œ, í‰ê·  ì†ì‹¤(loss): {avg_loss:.4f}")

    print("\nâœ… Fine-Tuning ì™„ë£Œ!")

    # âœ… ëª¨ë¸ ì €ì¥
    model.save_pretrained("./gemma-finetuned")
    tokenizer.save_pretrained("./gemma-finetuned")
    print("âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ!")

    # âœ… ëª¨ë¸ í…ŒìŠ¤íŠ¸
    model.eval()
    test_input = "íšŒì›ê°€ì… ì–´ë””ì„œ í•´?"
    inputs = tokenizer(test_input, return_tensors="pt").to(device)
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=50)
        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
    print("ğŸ”¹ ëª¨ë¸ ì¶œë ¥:", generated_text)

except Exception as e:
    print(f"âŒ í•™ìŠµ ì˜¤ë¥˜ ë°œìƒ: {e}")